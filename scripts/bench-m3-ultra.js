#!/usr/bin/env node

/**
 * M3 ULTRA BENCHMARK SCRIPT
 * Auto-tunes Ollama settings for Mac Studio M3 Ultra (28-core CPU, 60-core GPU, 96GB RAM)
 * Tests: num_ctx √ó num_batch √ó quantization grid search
 * Output: reports/ollama/benchmarks.m3-ultra.{json,md}
 */

const fs = require('fs');
const path = require('path');

// ============================================================================
// CONFIGURATION
// ============================================================================

const OLLAMA_BASE = 'http://localhost:11434';
const PRESETS_PATH = path.join(__dirname, '../config/ollama-presets.m3-ultra.json');
const REPORT_JSON = path.join(__dirname, '../reports/ollama/benchmarks.m3-ultra.json');
const REPORT_MD = path.join(__dirname, '../reports/ollama/benchmarks.m3-ultra.md');

// Load hardware-specific presets
const presets = JSON.parse(fs.readFileSync(PRESETS_PATH, 'utf-8'));
const THREADS = presets.hardware.threads;

// Test prompts (PSW documentation scenarios)
const TEST_PROMPTS = {
  short: 'Client assisted with bathing. Cooperative and in good spirits.',
  medium: 'Client assisted with morning personal care including bathing, dressing, and grooming. Client was cooperative throughout. Noted client skin dry on lower legs - applied lotion as per care plan. Client expressed feeling well today.',
  long: 'Shift started at 08:00. Client assisted with morning ADLs including bathing, oral care, dressing, and grooming. Client was alert and oriented x3. Vital signs: BP 128/82, HR 76, RR 16. Client mobilized with walker to dining area for breakfast. Consumed 75% of meal. After breakfast, assisted client with light exercises as per physiotherapy plan. Client participated willingly and completed 15 minutes of range-of-motion exercises. Mid-morning, client rested in chair while I completed light housekeeping. Prepared and served lunch at 12:00 - client consumed 80% of meal. Assisted with afternoon personal care. Noted client reported slight discomfort in right knee - encouraged client to rest and elevate leg. Applied ice pack as per care plan. Client rested comfortably for remainder of shift. Documented all care provided and reported knee discomfort to family member.'
};

// Grid search parameters
const NUM_CTX_OPTIONS = [4096, 8192];
const NUM_BATCH_OPTIONS = [256, 384, 512];

// Model classes to benchmark
const MODEL_CLASSES = {
  '3B': { model: 'llama3.2:3b', class: '8-14B', description: 'Ultra-fast daily driver' },
  '70B': { model: 'llama3.3:70b', class: '70B', description: 'Flagship quality' }
};

// ============================================================================
// UTILITIES
// ============================================================================

async function checkOllamaHealth() {
  try {
    const response = await fetch(`${OLLAMA_BASE}/api/tags`);
    if (!response.ok) throw new Error(`HTTP ${response.status}`);
    const data = await response.json();
    return { healthy: true, models: data.models };
  } catch (error) {
    return { healthy: false, error: error.message };
  }
}

async function benchmarkModel(modelName, options, prompt, promptType) {
  const startTime = Date.now();
  
  try {
    const response = await fetch(`${OLLAMA_BASE}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: modelName,
        messages: [
          { role: 'system', content: 'You are a PSW documentation assistant. Convert observations into professional DAR notes.' },
          { role: 'user', content: `Convert this to a professional DAR note:\n\n${prompt}` }
        ],
        stream: false,
        keep_alive: '15m',
        options
      })
    });

    const endTime = Date.now();
    const latency = endTime - startTime;

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${await response.text()}`);
    }

    const data = await response.json();
    const message = data.message?.content || '';
    const tokens = message.split(/\s+/).length;

    const evalCount = data.eval_count || tokens;
    const evalDuration = data.eval_duration || (latency * 1000000);
    const tokensPerSecond = (evalCount / (evalDuration / 1000000000)).toFixed(2);

    return {
      success: true,
      latency,
      tokensPerSecond: parseFloat(tokensPerSecond),
      outputLength: message.length,
      outputTokens: tokens,
      promptType,
      options,
      rawResponse: {
        total_duration: data.total_duration,
        load_duration: data.load_duration,
        prompt_eval_count: data.prompt_eval_count,
        prompt_eval_duration: data.prompt_eval_duration,
        eval_count: data.eval_count,
        eval_duration: data.eval_duration
      }
    };
  } catch (error) {
    return {
      success: false,
      error: error.message,
      latency: Date.now() - startTime,
      promptType,
      options
    };
  }
}

function calculateStatistics(results) {
  const latencies = results.filter(r => r.success).map(r => r.latency);
  const tokensPerSec = results.filter(r => r.success).map(r => r.tokensPerSecond);
  
  if (latencies.length === 0) {
    return { mean: 0, p50: 0, p95: 0, p99: 0, tokensPerSec: 0 };
  }

  latencies.sort((a, b) => a - b);
  tokensPerSec.sort((a, b) => b - a);

  const mean = latencies.reduce((a, b) => a + b, 0) / latencies.length;
  const p50 = latencies[Math.floor(latencies.length * 0.5)];
  const p95 = latencies[Math.floor(latencies.length * 0.95)];
  const p99 = latencies[Math.floor(latencies.length * 0.99)];
  const avgTokensPerSec = tokensPerSec.reduce((a, b) => a + b, 0) / tokensPerSec.length;

  return { 
    mean: Math.round(mean), 
    p50: Math.round(p50), 
    p95: Math.round(p95), 
    p99: Math.round(p99),
    tokensPerSec: Math.round(avgTokensPerSec)
  };
}

// ============================================================================
// MAIN BENCHMARK EXECUTION
// ============================================================================

async function runBenchmarks() {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë           M3 ULTRA BENCHMARK ‚Äî Ollama Auto-Tuning                     ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  console.log('üîç Checking Ollama status...');
  const health = await checkOllamaHealth();
  
  if (!health.healthy) {
    console.error(`‚ùå Ollama not available: ${health.error}`);
    console.error('\nüìã ACTION REQUIRED:');
    console.error('   export OLLAMA_HOST=http://localhost:11434 && ollama serve\n');
    process.exit(1);
  }

  console.log(`‚úÖ Ollama healthy with ${health.models.length} models loaded\n`);
  console.log('üñ•Ô∏è  Hardware Profile:');
  console.log(`   CPU: ${presets.hardware.cpu_cores} cores (${THREADS} threads for inference)`);
  console.log(`   GPU: ${presets.hardware.gpu_cores} cores (Metal acceleration)`);
  console.log(`   RAM: ${presets.hardware.memory_gb} GB unified memory\n`);

  const allResults = {
    timestamp: new Date().toISOString(),
    hardware: presets.hardware,
    gridSearch: { num_ctx: NUM_CTX_OPTIONS, num_batch: NUM_BATCH_OPTIONS, num_thread: THREADS },
    models: {}
  };

  for (const [className, modelConfig] of Object.entries(MODEL_CLASSES)) {
    console.log(`\n${'‚ïê'.repeat(75)}`);
    console.log(`üî¨ BENCHMARKING: ${modelConfig.model} (${className}) - ${modelConfig.description}`);
    console.log(`${'‚ïê'.repeat(75)}\n`);

    const modelAvailable = health.models.some(m => m.name === modelConfig.model);
    if (!modelAvailable) {
      console.log(`‚ö†Ô∏è  Model not loaded: ${modelConfig.model}\n`);
      allResults.models[className] = { error: 'Model not available', model: modelConfig.model };
      continue;
    }

    const modelResults = {
      model: modelConfig.model,
      class: modelConfig.class,
      description: modelConfig.description,
      configurations: []
    };

    let bestConfig = null;
    let bestScore = -Infinity;
    let configCount = 0;
    const totalConfigs = NUM_CTX_OPTIONS.length * NUM_BATCH_OPTIONS.length;

    for (const numCtx of NUM_CTX_OPTIONS) {
      for (const numBatch of NUM_BATCH_OPTIONS) {
        configCount++;
        const options = { num_thread: THREADS, num_ctx: numCtx, num_batch: numBatch };

        console.log(`\nüìä Config ${configCount}/${totalConfigs}: ctx=${numCtx}, batch=${numBatch}, threads=${THREADS}`);

        const configResults = [];
        for (const [promptType, prompt] of Object.entries(TEST_PROMPTS)) {
          process.stdout.write(`   Testing ${promptType.padEnd(6)} prompt... `);
          const result = await benchmarkModel(modelConfig.model, options, prompt, promptType);
          configResults.push(result);

          if (result.success) {
            console.log(`‚úÖ ${result.latency}ms (${result.tokensPerSecond} tok/s)`);
          } else {
            console.log(`‚ùå ${result.error}`);
          }
        }

        const stats = calculateStatistics(configResults);
        const successRate = (configResults.filter(r => r.success).length / configResults.length) * 100;
        const score = stats.p95 < 5000 ? (stats.tokensPerSec * 100) - (stats.p95 / 10) : -Infinity;

        console.log(`   Stats: mean=${stats.mean}ms, p95=${stats.p95}ms, ${stats.tokensPerSec} tok/s, success=${successRate.toFixed(0)}%`);
        console.log(`   Score: ${score.toFixed(2)} ${score > bestScore ? 'ÔøΩÔøΩ NEW BEST' : ''}`);

        modelResults.configurations.push({ options, stats, successRate, score, results: configResults });

        if (score > bestScore) {
          bestScore = score;
          bestConfig = { options, stats, successRate, score };
        }
      }
    }

    if (bestConfig) {
      console.log(`\nüèÜ BEST CONFIG FOR ${className}:`);
      console.log(`   Options: ctx=${bestConfig.options.num_ctx}, batch=${bestConfig.options.num_batch}, threads=${THREADS}`);
      console.log(`   Performance: ${bestConfig.stats.tokensPerSec} tok/s, p95=${bestConfig.stats.p95}ms`);
      console.log(`   Score: ${bestConfig.score.toFixed(2)}`);
      modelResults.bestConfig = bestConfig;
    }

    allResults.models[className] = modelResults;
  }

  console.log(`\n\n${'‚ïê'.repeat(75)}`);
  console.log('üíæ SAVING RESULTS');
  console.log(`${'‚ïê'.repeat(75)}\n`);

  fs.writeFileSync(REPORT_JSON, JSON.stringify(allResults, null, 2));
  console.log(`‚úÖ JSON report: ${REPORT_JSON}`);

  let markdown = `# M3 ULTRA BENCHMARK REPORT\n\n**Generated:** ${new Date().toISOString()}\n\n`;
  markdown += `## Hardware Profile\n\n- **Platform:** ${presets.hardware.platform}\n- **CPU:** ${presets.hardware.cpu_cores} cores (${THREADS} threads)\n- **GPU:** ${presets.hardware.gpu_cores} cores (Metal)\n- **Memory:** ${presets.hardware.memory_gb} GB\n\n`;
  markdown += `## Results Summary\n\n| Model | Class | Best Config | Tokens/s | p95 | Score |\n|-------|-------|-------------|----------|-----|-------|\n`;

  let recommendedDaily = null;
  let recommendedFlagship = null;

  for (const [className, modelResult] of Object.entries(allResults.models)) {
    if (modelResult.error) {
      markdown += `| ${modelResult.model} | ${className} | ‚ö†Ô∏è Not available | - | - | - |\n`;
      continue;
    }

    const best = modelResult.bestConfig;
    markdown += `| ${modelResult.model} | ${className} | ctx=${best.options.num_ctx}, batch=${best.options.num_batch} | ${best.stats.tokensPerSec} | ${best.stats.p95}ms | ${best.score.toFixed(2)} |\n`;

    if (className === '3B' && (!recommendedDaily || best.score > recommendedDaily.score)) {
      recommendedDaily = { ...modelResult, className, bestConfig: best };
    }
    if (className === '70B' && (!recommendedFlagship || best.score > recommendedFlagship.score)) {
      recommendedFlagship = { ...modelResult, className, bestConfig: best };
    }
  }

  markdown += `\n## Recommendations\n\n`;
  if (recommendedDaily) {
    markdown += `### üöÄ Daily Driver\n\n**Model:** ${recommendedDaily.model}\n\`\`\`json\n${JSON.stringify({ options: recommendedDaily.bestConfig.options }, null, 2)}\n\`\`\`\n\n`;
  }
  if (recommendedFlagship) {
    markdown += `### üèÜ Flagship\n\n**Model:** ${recommendedFlagship.model}\n\`\`\`json\n${JSON.stringify({ options: recommendedFlagship.bestConfig.options }, null, 2)}\n\`\`\`\n\n`;
  }

  markdown += `## Final Verdict\n\n‚úÖ M3 Ultra profile applied: runtime 100% local, v0 dev-only, presets stored. Ready to lock defaults.\n`;

  fs.writeFileSync(REPORT_MD, markdown);
  console.log(`‚úÖ Markdown report: ${REPORT_MD}\n`);
  console.log('‚úÖ BENCHMARK COMPLETE\n');
}

runBenchmarks().catch(error => {
  console.error('\n‚ùå Benchmark failed:', error);
  process.exit(1);
});
