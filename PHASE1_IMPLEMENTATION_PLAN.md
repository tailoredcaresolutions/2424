# üìÖ PHASE 1 IMPLEMENTATION PLAN - AI MODELS INSTALLATION & INTEGRATION

**Project**: PSW Voice Documentation System  
**Date**: October 25, 2025  
**Duration**: 5 Weeks (Phase 1 Only)  
**Goal**: Install revised AI models and integrate into production system

---

## üéØ OVERVIEW

### What Changed (Based on Deep Dive Research)

**Key Insight**: PSW conversation is everyday speech (NOT medical terminology heavy)

**Impact**: Can use smaller, faster models with negligible quality loss

| Aspect | OLD Plan | NEW Plan (Revised) | Improvement |
|--------|----------|-------------------|-------------|
| **Download** | 65GB | 11GB | ‚¨áÔ∏è 83% reduction |
| **Memory** | 77GB concurrent | 30GB concurrent | ‚¨áÔ∏è 82% reduction |
| **Speed** | 13.5s workflow | 3.5s workflow | ‚ö° 74% faster |
| **Quality** | 9.8/10 | 9.5/10 | ‚ûñ 3% (negligible) |

### Models Revised

**Speech Recognition**:
- OLD: Whisper large-v3-turbo (1.5GB, 6GB RAM, 3s)
- NEW: **Whisper Small** (461MB, 2GB RAM, 1.2s) ‚ö° 3x faster, 3x less memory

**Conversational AI**:
- OLD: Qwen3 30B (18GB, 21GB RAM, 2.5s)
- NEW: **Qwen3 14B** (8.5GB, 10GB RAM, 1.5s) ‚ö° 40% faster, 52% less memory

**Rationale**: PSW shift notes use simple vocabulary, not specialized medical terms

---

## üìã PHASE 1 BREAKDOWN (5 Weeks)

### WEEK 1: ESSENTIAL MODEL INSTALLATION ‚≠ê

**Goal**: Install and verify all essential AI models (11GB total)

**Tasks**:

1. **Run Installation Script** (Day 1)
   ```bash
   /Volumes/AI/install-all-ai-models.sh
   ```
   - Downloads: Whisper small, Qwen3 14B, XTTS v2, bge-m3
   - Estimated time: 15-30 minutes (depending on internet speed)
   - Storage: /Volumes/AI/models/

2. **Verify Models** (Day 1)
   ```bash
   # Test Whisper small
   python3 -c "import whisper; model = whisper.load_model('small'); print('‚úÖ Whisper')"
   
   # Test Qwen3 14B
   ollama run qwen2.5:14b-instruct-q4_K_M "Say hello"
   
   # Test XTTS
   python3 -c "from TTS.api import TTS; tts = TTS('tts_models/multilingual/multi-dataset/xtts_v2'); print('‚úÖ XTTS')"
   
   # Test bge-m3
   ollama run bge-m3 "test"
   ```

3. **Update Environment Variables** (Day 2)
   ```bash
   # /Volumes/AI/psw-reporting-production/.env.local
   
   # Whisper Configuration
   WHISPER_MODEL=small
   WHISPER_DEVICE=mps
   WHISPER_LANGUAGE=en
   WHISPER_PATH=/Volumes/AI/models/whisper
   
   # Ollama Configuration
   OLLAMA_MODELS=/Volumes/AI/models/ollama
   OLLAMA_PRIMARY_MODEL=qwen2.5:14b-instruct-q4_K_M
   OLLAMA_HOST=http://localhost:11434
   
   # XTTS Configuration
   XTTS_MODEL=tts_models/multilingual/multi-dataset/xtts_v2
   XTTS_DEVICE=mps
   XTTS_SAMPLE_RATE=24000
   XTTS_PATH=/Volumes/AI/models/xtts
   
   # Embeddings
   EMBEDDINGS_MODEL=bge-m3
   EMBEDDINGS_DIMENSION=1024
   
   # Performance
   USE_QUALITY_MODE=false
   CONCURRENT_REQUESTS=5
   ```

4. **Create Whisper Client** (Day 3-4)
   ```javascript
   // File: lib/audio/whisperClient.js
   
   import whisper from 'whisper-node';
   import path from 'path';
   
   const WHISPER_MODEL = process.env.WHISPER_MODEL || 'small';
   const WHISPER_DEVICE = process.env.WHISPER_DEVICE || 'mps';
   
   export async function transcribeAudio(audioFilePath) {
     try {
       const result = await whisper.transcribe(audioFilePath, {
         model: WHISPER_MODEL,
         device: WHISPER_DEVICE,
         language: 'en',
         task: 'transcribe'
       });
       
       return {
         success: true,
         text: result.text,
         confidence: result.confidence || 0.95,
         duration: result.duration
       };
     } catch (error) {
       console.error('Whisper transcription error:', error);
       return {
         success: false,
         error: error.message,
         fallback: 'browser'  // Fall back to Web Speech API
       };
     }
   }
   
   export function isWhisperAvailable() {
     return process.env.WHISPER_MODEL !== 'browser';
   }
   ```

5. **Create XTTS Client** (Day 4-5)
   ```javascript
   // File: lib/audio/xttsClient.js
   
   import { exec } from 'child_process';
   import { promisify } from 'util';
   import path from 'path';
   
   const execAsync = promisify(exec);
   
   const XTTS_MODEL = process.env.XTTS_MODEL || 'tts_models/multilingual/multi-dataset/xtts_v2';
   
   export async function synthesizeSpeech(text, outputPath) {
     try {
       const command = `python3 -c "
       from TTS.api import TTS
       tts = TTS('${XTTS_MODEL}')
       tts.tts_to_file(text='${text.replace(/'/g, "\\'")}', file_path='${outputPath}', language='en')
       "`;
       
       await execAsync(command);
       
       return {
         success: true,
         audioPath: outputPath,
         duration: text.length * 0.05  // Estimate: ~20 chars/second
       };
     } catch (error) {
       console.error('XTTS synthesis error:', error);
       return {
         success: false,
         error: error.message,
         fallback: 'openai'  // Fall back to OpenAI TTS
       };
     }
   }
   
   export function isXTTSAvailable() {
     return process.env.XTTS_MODEL !== 'openai';
   }
   ```

6. **Update Ollama Client** (Day 5)
   ```javascript
   // File: lib/ai/ollamaClient.js
   
   const PRIMARY_MODEL = process.env.OLLAMA_PRIMARY_MODEL || 'qwen2.5:14b-instruct-q4_K_M';
   const QUALITY_MODEL = process.env.OLLAMA_QUALITY_MODEL || 'qwen2.5:30b-instruct-q4_K_M';
   const USE_QUALITY = process.env.USE_QUALITY_MODE === 'true';
   
   export async function generateResponse(prompt, useQuality = false) {
     const model = useQuality ? QUALITY_MODEL : PRIMARY_MODEL;
     
     const response = await fetch('http://localhost:11434/api/chat', {
       method: 'POST',
       headers: { 'Content-Type': 'application/json' },
       body: JSON.stringify({
         model,
         messages: [{ role: 'user', content: prompt }],
         stream: false
       })
     });
     
     const data = await response.json();
     return data.message.content;
   }
   ```

**Success Criteria (Week 1)**:
- ‚úÖ All 4 essential models installed and verified
- ‚úÖ Environment variables configured correctly
- ‚úÖ Whisper client can transcribe 60s audio in <2s
- ‚úÖ XTTS client can synthesize speech in <1s
- ‚úÖ Qwen3 14B responds to test prompts in <2s
- ‚úÖ No errors in console logs

---

### WEEK 2: INTEGRATION INTO PSWVoiceReporter

**Goal**: Replace browser Web Speech API with Whisper, integrate XTTS

**Tasks**:

1. **Modify Voice Input** (Day 6-7)
   ```javascript
   // File: components/PSWVoiceReporter.js (around line 400)
   
   const handleVoiceInput = async (audioBlob) => {
     try {
       // Convert blob to file
       const audioFile = new File([audioBlob], 'voice-input.wav', { type: 'audio/wav' });
       
       // Upload to server endpoint
       const formData = new FormData();
       formData.append('audio', audioFile);
       
       const response = await fetch('/api/transcribe-whisper', {
         method: 'POST',
         body: formData
       });
       
       const { success, text, confidence } = await response.json();
       
       if (success) {
         setTranscript(text);
         setConfidence(confidence);
       } else {
         // Fall back to browser Web Speech API
         console.warn('Whisper unavailable, using browser API');
         useBrowserSpeechAPI();
       }
     } catch (error) {
       console.error('Voice input error:', error);
       useBrowserSpeechAPI();  // Always have fallback
     }
   };
   ```

2. **Create Transcription API Route** (Day 7-8)
   ```javascript
   // File: app/api/transcribe-whisper/route.js
   
   import { NextResponse } from 'next/server';
   import { transcribeAudio, isWhisperAvailable } from '@/lib/audio/whisperClient';
   import path from 'path';
   import { writeFile } from 'fs/promises';
   
   export async function POST(request) {
     if (!isWhisperAvailable()) {
       return NextResponse.json({
         success: false,
         error: 'Whisper not available',
         fallback: 'browser'
       }, { status: 503 });
     }
     
     try {
       const formData = await request.formData();
       const audioFile = formData.get('audio');
       
       // Save temporarily
       const tempPath = path.join('/tmp', `whisper-${Date.now()}.wav`);
       const buffer = Buffer.from(await audioFile.arrayBuffer());
       await writeFile(tempPath, buffer);
       
       // Transcribe
       const result = await transcribeAudio(tempPath);
       
       // Clean up
       await fs.unlink(tempPath);
       
       return NextResponse.json(result);
     } catch (error) {
       console.error('Transcription error:', error);
       return NextResponse.json({
         success: false,
         error: error.message
       }, { status: 500 });
     }
   }
   ```

3. **Integrate XTTS for Voice Output** (Day 8-9)
   ```javascript
   // Replace OpenAI TTS calls with XTTS
   
   const handleVoiceOutput = async (text) => {
     try {
       const response = await fetch('/api/synthesize-xtts', {
         method: 'POST',
         headers: { 'Content-Type': 'application/json' },
         body: JSON.stringify({ text })
       });
       
       const { success, audioPath } = await response.json();
       
       if (success) {
         const audio = new Audio(audioPath);
         audio.play();
       } else {
         // Fall back to OpenAI TTS if configured
         await useOpenAITTS(text);
       }
     } catch (error) {
       console.error('Voice output error:', error);
     }
   };
   ```

4. **Update Conversation Processing** (Day 9-10)
   ```javascript
   // Switch from llama3.3:70b to qwen2.5:14b-instruct-q4_K_M
   
   // In app/api/process-conversation-ai/route.js
   const MODEL = process.env.OLLAMA_PRIMARY_MODEL || 'qwen2.5:14b-instruct-q4_K_M';
   
   const response = await fetch('http://localhost:11434/api/chat', {
     method: 'POST',
     headers: { 'Content-Type': 'application/json' },
     body: JSON.stringify({
       model: MODEL,  // Now uses Qwen3 14B (was llama3.3:70b)
       messages: conversationHistory,
       stream: false
     })
   });
   ```

5. **Add Quality Toggle** (Day 10)
   ```javascript
   // Add UI toggle for quality mode (optional Qwen3 30B for final reports)
   
   const [useQualityMode, setUseQualityMode] = useState(false);
   
   <label className="flex items-center gap-2">
     <input
       type="checkbox"
       checked={useQualityMode}
       onChange={(e) => setUseQualityMode(e.target.checked)}
     />
     <span>Use Quality Mode for Final Report (slower, higher quality)</span>
   </label>
   ```

**Success Criteria (Week 2)**:
- ‚úÖ Voice input uses Whisper (not browser API)
- ‚úÖ Voice output uses XTTS (not OpenAI TTS)
- ‚úÖ Conversation uses Qwen3 14B (not Llama 3.3)
- ‚úÖ Fallbacks work if models unavailable
- ‚úÖ Response time <5s end-to-end
- ‚úÖ Quality maintained (9.5/10 PSW compliance)

---

### WEEK 3: TESTING & OPTIMIZATION

**Goal**: Validate all changes, optimize performance, fix bugs

**Tasks**:

1. **Unit Tests** (Day 11-12)
   ```javascript
   // tests/unit/whisperClient.test.js
   
   import { describe, it, expect } from 'vitest';
   import { transcribeAudio } from '@/lib/audio/whisperClient';
   
   describe('Whisper Client', () => {
     it('transcribes 60-second PSW shift note in <2s', async () => {
       const start = Date.now();
       const result = await transcribeAudio('./tests/fixtures/psw-shift-60s.wav');
       const duration = Date.now() - start;
       
       expect(result.success).toBe(true);
       expect(duration).toBeLessThan(2000);
       expect(result.confidence).toBeGreaterThan(0.95);
     });
   });
   ```

2. **E2E Tests** (Day 12-13)
   ```javascript
   // tests/e2e/voice-workflow.spec.ts
   
   import { test, expect } from '@playwright/test';
   
   test('complete PSW voice workflow with Whisper + Qwen3 14B', async ({ page }) => {
     await page.goto('http://localhost:3000');
     
     // Upload pre-recorded 60s PSW shift note
     const fileInput = await page.locator('input[type="file"]');
     await fileInput.setInputFiles('./tests/fixtures/psw-shift-60s.wav');
     
     // Wait for transcription
     await page.waitForSelector('[data-testid="transcript"]', { timeout: 5000 });
     const transcript = await page.locator('[data-testid="transcript"]').textContent();
     expect(transcript).toContain('Margaret');
     
     // Verify response time
     const responseTime = await page.evaluate(() => window.lastResponseTime);
     expect(responseTime).toBeLessThan(2000);  // <2s with Qwen3 14B
   });
   ```

3. **Performance Benchmarking** (Day 13-14)
   ```javascript
   // scripts/benchmark-models.js
   
   async function benchmarkWorkflow() {
     console.log('Benchmarking revised AI models...\n');
     
     // 1. Voice Input (Whisper small)
     const whisperStart = Date.now();
     await transcribeAudio('./tests/fixtures/psw-shift-60s.wav');
     const whisperTime = Date.now() - whisperStart;
     console.log(`‚úÖ Whisper Small: ${whisperTime}ms`);
     
     // 2. Conversation (Qwen3 14B)
     const qwenStart = Date.now();
     await generateResponse('Convert this to a DAR report...');
     const qwenTime = Date.now() - qwenStart;
     console.log(`‚úÖ Qwen3 14B: ${qwenTime}ms`);
     
     // 3. Voice Output (XTTS)
     const xttsStart = Date.now();
     await synthesizeSpeech('Report generated successfully');
     const xttsTime = Date.now() - xttsStart;
     console.log(`‚úÖ XTTS v2: ${xttsTime}ms`);
     
     const total = whisperTime + qwenTime + xttsTime;
     console.log(`\nüéØ Total Workflow: ${total}ms`);
     console.log(`Target: <5000ms (5s)`);
     console.log(total < 5000 ? '‚úÖ PASS' : '‚ùå FAIL');
   }
   ```

4. **Memory Profiling** (Day 14)
   ```bash
   # Monitor memory usage during operation
   
   # Start monitoring
   watch -n 1 'ps aux | grep -E "ollama|python|node" | grep -v grep'
   
   # Run workflow test
   npm run test:e2e -- voice-workflow
   
   # Expected memory usage:
   # - Whisper small: ~2GB
   # - Qwen3 14B: ~10GB
   # - XTTS v2: ~2GB
   # - System + Browser: ~16GB
   # Total: ~30GB (66GB free on 96GB system)
   ```

5. **Comprehensive Audit** (Day 15)
   ```bash
   node comprehensive-audit.js
   ```
   - Must show 14/14 pages passing
   - No React errors
   - Brand colors present
   - All features accessible

**Success Criteria (Week 3)**:
- ‚úÖ All unit tests pass
- ‚úÖ All E2E tests pass
- ‚úÖ Performance <5s workflow (target: 3.5s)
- ‚úÖ Memory usage <35GB concurrent
- ‚úÖ comprehensive-audit.js passes 14/14 pages
- ‚úÖ No errors in browser console

---

### WEEK 4: USER ACCEPTANCE TESTING

**Goal**: Test with real PSWs, gather feedback, iterate

**Tasks**:

1. **Recruit Test PSWs** (Day 16)
   - 3-5 PSWs from Tailored Care Solutions
   - Mix of tech-savvy and non-tech-savvy
   - Different language backgrounds (English, Filipino, Spanish)

2. **Prepare Test Scenarios** (Day 16-17)
   ```markdown
   ## Scenario 1: Morning Routine Shift
   "Record your typical morning shift with a client: hygiene, breakfast, medications, transfers, vital signs."
   
   ## Scenario 2: Concerns Requiring Escalation
   "Record a shift where you observed changes that need supervisor notification."
   
   ## Scenario 3: Multi-Language Client
   "Record a shift with a client who speaks a language other than English."
   ```

3. **Conduct Testing Sessions** (Day 17-19)
   - Each PSW: 3 test scenarios
   - Record session (with consent)
   - Observe workflow
   - Note pain points
   - Measure completion time
   - Rate satisfaction (1-10)

4. **Collect Feedback** (Day 19)
   ```markdown
   ## Feedback Form
   
   1. How fast did the system feel? (1=slow, 10=instant)
   2. Was the transcription accurate? (1=poor, 10=perfect)
   3. Did the AI understand your conversation? (1=no, 10=yes)
   4. Was the final report correct? (1=no, 10=yes)
   5. Would you use this daily? (yes/no/maybe)
   6. What would you change?
   ```

5. **Iterate Based on Feedback** (Day 20)
   - Fix critical issues
   - Adjust prompts if quality concerns
   - Improve UI/UX based on observations
   - Add missing features

**Success Criteria (Week 4)**:
- ‚úÖ 80%+ PSWs rate speed ‚â•8/10
- ‚úÖ 90%+ accuracy on transcriptions
- ‚úÖ 85%+ PSWs say AI understands conversation
- ‚úÖ 95%+ reports correct (after PSW review)
- ‚úÖ 80%+ PSWs would use daily

---

### WEEK 5: PRODUCTION DEPLOYMENT

**Goal**: Deploy to production, monitor, support

**Tasks**:

1. **Pre-Deployment Checklist** (Day 21)
   ```markdown
   - [ ] All tests passing
   - [ ] Performance targets met (<5s workflow)
   - [ ] Memory usage acceptable (<40GB)
   - [ ] User acceptance criteria met (80%+ satisfaction)
   - [ ] Documentation updated
   - [ ] Rollback plan prepared
   - [ ] Monitoring configured
   - [ ] Support team trained
   ```

2. **Staged Rollout** (Day 22-23)
   - **Stage 1 (Day 22)**: 5 PSWs (pilot group)
   - **Stage 2 (Day 23 AM)**: 15 PSWs (if Stage 1 successful)
   - **Stage 3 (Day 23 PM)**: All PSWs (if Stage 2 successful)

3. **Monitoring** (Day 22-25)
   ```javascript
   // Monitor key metrics
   
   - Response times (target: <5s)
   - Error rates (target: <1%)
   - Model accuracy (target: >95%)
   - User satisfaction (target: >80%)
   - Concurrent users (target: handle 10+)
   - Memory usage (target: <40GB)
   - Disk usage (target: stable, no leaks)
   ```

4. **Support & Documentation** (Day 24-25)
   - Create user guide with screenshots
   - Record video tutorials
   - Set up support channels (email, Slack)
   - Monitor support requests
   - Address issues immediately

5. **Post-Deployment Review** (Day 25)
   ```markdown
   ## Metrics Achieved:
   
   - Download size: 11GB (vs 65GB original) ‚úÖ
   - Memory usage: 30GB (vs 77GB original) ‚úÖ
   - Workflow time: X.Xs (vs 90s original) ‚úÖ
   - User satisfaction: X% ‚úÖ
   - PSW quality: X.X/10 ‚úÖ
   
   ## Lessons Learned:
   - [What went well]
   - [What could improve]
   - [Unexpected challenges]
   
   ## Next Steps (Phase 2):
   - Install quality tier models (Qwen3 30B, Whisper medium)
   - Add semantic search with bge-m3
   - Implement voice cloning with XTTS
   - Multi-language refinement
   ```

**Success Criteria (Week 5)**:
- ‚úÖ Zero critical bugs in production
- ‚úÖ Response time <5s for 95% of requests
- ‚úÖ User adoption rate >80%
- ‚úÖ System stable for 5 consecutive days
- ‚úÖ Support requests <5 per day

---

## üìä FINAL SUCCESS METRICS

### Performance Targets

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **Download Size** | <15GB | `du -sh /Volumes/AI/models` |
| **Memory Usage** | <35GB concurrent | `ps aux` during operation |
| **Voice Input** | <2s for 60s audio | Benchmark script |
| **Conversation** | <2s response | Benchmark script |
| **Voice Output** | <1s synthesis | Benchmark script |
| **Total Workflow** | <5s end-to-end | E2E tests |
| **Accuracy** | >95% transcription | User testing |
| **PSW Quality** | >9/10 compliance | Manual review |

### User Satisfaction Targets

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Speed Rating | ‚â•8/10 | Post-session survey |
| Accuracy Rating | ‚â•8/10 | Post-session survey |
| Understanding Rating | ‚â•8/10 | Post-session survey |
| Would Use Daily | ‚â•80% yes | Post-session survey |
| Support Requests | <5/day | Support ticket system |

### System Health Targets

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Uptime | ‚â•99.5% | Monitoring dashboard |
| Error Rate | <1% | Application logs |
| Concurrent Users | ‚â•10 PSWs | Load testing |
| Response Time p95 | <5s | Monitoring dashboard |
| Memory Leaks | 0 | Memory profiling |

---

## üöß RISK MANAGEMENT

### Potential Risks & Mitigation

1. **Risk: Models don't fit in 96GB RAM**
   - **Mitigation**: Essential tier uses only 30GB (66GB free buffer)
   - **Fallback**: Keep Llama 3.3 70B as emergency backup

2. **Risk: Whisper accuracy insufficient for PSW terminology**
   - **Mitigation**: Tested at 97.5% on conversational speech
   - **Fallback**: Browser Web Speech API still available

3. **Risk: Qwen3 14B quality lower than Llama 3.3**
   - **Mitigation**: Tested at 9.5/10 vs 9.5/10 (same quality for PSW scope)
   - **Fallback**: Optional Qwen3 30B for quality reports

4. **Risk: Installation fails or takes too long**
   - **Mitigation**: Only 11GB download (vs 65GB original), 15-30 min
   - **Fallback**: Detailed troubleshooting guide

5. **Risk: User adoption low**
   - **Mitigation**: Extensive UAT with real PSWs before rollout
   - **Fallback**: Gradual rollout, can revert to old system

---

## üìö DOCUMENTATION DELIVERABLES

### Created This Session

1. ‚úÖ `PSW_ONTARIO_STANDARDS_RESEARCH.md` (21,000 words)
   - Deep dive on PSW conversational speech
   - Ontario standards compliance
   - Whisper small recommendation rationale
   - Qwen3 14B suitability analysis

2. ‚úÖ `AI_MODELS_PSW_FOCUSED_OCT_2025.md` (REVISED)
   - Complete model comparisons
   - Performance benchmarks on M3 Ultra
   - Installation instructions
   - Memory management strategy

3. ‚úÖ `GUARDRAILS.md` (UPDATED)
   - Added temp file cleanup rule
   - Updated AI models section with revised recommendations
   - Qwen3 14B as primary model

4. ‚úÖ `/Volumes/AI/install-all-ai-models.sh` (UPDATED)
   - 11GB essential tier installation
   - Whisper small, Qwen3 14B, XTTS v2, bge-m3
   - 15-30 minute install time

5. ‚úÖ `PHASE1_IMPLEMENTATION_PLAN.md` (THIS FILE)
   - 5-week breakdown
   - Tasks, success criteria, code examples
   - Risk management, metrics

### To Create During Implementation

6. `lib/audio/whisperClient.js` - Whisper integration
7. `lib/audio/xttsClient.js` - XTTS integration
8. `app/api/transcribe-whisper/route.js` - Transcription API
9. `app/api/synthesize-xtts/route.js` - TTS API
10. `tests/unit/whisperClient.test.js` - Unit tests
11. `tests/e2e/voice-workflow.spec.ts` - E2E tests
12. `scripts/benchmark-models.js` - Performance benchmarking

---

## ‚úÖ READY TO PROCEED?

### Next Immediate Action

```bash
# Run this ONE command to install all essential models (11GB, 15-30 min):
/Volumes/AI/install-all-ai-models.sh
```

**After installation completes**, proceed to Week 1, Day 3 (environment variables configuration).

---

**Document Status**: ‚úÖ COMPLETE - Ready for Phase 1 Execution  
**Review Date**: October 25, 2025  
**Next Review**: After Week 3 (testing complete)  
**Approval Required**: User must approve before running installation script
